* Ideas
** Kasm workspace gaming rig?
* Notes
** nmap 
*** types of scans
**** ~SYN~ scan (default scan type)
A ~SYN~ scan will cause =nmap= to send a syn ~TCP~ message to see if a given machine will respond with ~ACK~.
=nmap= will not send an ~ACK~ back.
**** Version scan ~-sV~ 
The ~-sV~ parameter will tell =nmap= to attempt to identify versions of services that are running.
**** Detailed scan ~-sC~
The ~-sC~ parameter tells =nmap= to gather more information during the scan.
It will gather more information by following a script.
There are many scripts available, but if you do not specify a scrip it will use the default.
The default script is a little more verbose, and provides some generally useful information.
More scripts are available at =/usr/share/nmap/scripts/=.
Other scripts can be specified by ~-sC $scriptname~.
**** All ports scan ~-p-~
The ~-p-~ parameter tells nmap to scan all ports 1-65535.

*** examples
This will perform a ~SYN~ scan on =$IP=, for the top 1000 most common ports.
#+begin_src shell :results output
nmap 127.0.0.1
#+end_src

This example will scan for hosts running sort of ~SMB~ service.
#+begin_src shell :results output 
nmap -sC smb-os-discover.nse 10.10.10.0/24
#+end_src

** ssh-agent
=ssh-agent= uses environment variables to work.
Usually you'd have to export those variables manually.
You can run =ssh-agent= as a wrapper around another command,
which will set the environment variables and then spawn a sub-process with
those variables already set.
A useful example of this would be putting it in you x session file like below.

#+begin_src conf

[Desktop Entry]
Version=1.0
Type=Application
Name=Xmonad
Comment=Lightweight X11 tiled window manager written in Haskell
Exec=ssh-agent xmonad
Icon=xmonad
Terminal=false
StartupNotify=false
Categories=Application;

#+end_src
** proxmox
*** create Windows VM with GFX card
Here are the steps to take to creat a Windows VM on Proxmox
with a graphics card passed through.

1. Verify UEFI is setup to allow IOMMU
2. Edit =/etc/default/grub=
   Add ~amd_iommu~ or ~intel_iommu~ accordingly.
    #+BEGIN_SRC sh
      #from
      GRUB_CMDLINE_LINUX_DEFAULT="quiet"
      #to
      GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on"
    #+END_SRC
3. Run =grub-update=
4. Reboot and verify IOMMU is turned on successfully
   =dmesg | grep iommu=
5. Edit =/etc/modules=
   To make sure we use the dummy drivers/kernel-modules add the following
   to =/etc/modules=
   1. ~vfio~
   2. ~vfio_iommu_type1~
   3. ~vfio_pci~
   4. ~vfio_virqfd~
6. Edit =/etc/modprobe.d/pve-blacklist.conf=
   Add the following to =/etc/modprobe.d/pve-blacklist.conf= to stop the kernel
   from loading actual drivers instead of dummy drivers.
  1. ~blacklist nvidiafb~
  2. ~blacklist nvidia~
  3. ~blacklist nouveau~
  4. ~blacklist radeon~
      The ~radeon~ entry may need to be left out if you're using the iGPU
      on an AMD processor.
7. Create VM and install Windows
   + use UEFI instead of BIOS
   + use q35 machine type
   + add at least 8GB RAM
   + 8 CPU threads is preferable, may be able to do less
   + using virtio disk ensures best performace from the storage device
   + using virtio network adapter ensures best performance from the network adapter
   + make sure to install virtio Windows drivers
   + make sure RDP is enabled
      + you'll be unable to use Proxmox's vnc console
8. Edit =/etc/pve/qemu-server/$VMID.conf=
   To hide from Windows you're running it in a VM,
   add these flags to the ~cpu:~ line.
   =cpu: host,hidden=1,flags=+pcid=
9. Add GPU to VM using Proxmox GUI
   + check all boxes
     + All Functions
     + ROM-BAR
     + Primary GPU
     + PCI-Express
10. Edit =/etc/pve/qemu-server/$VMID.conf= again
    To allow the VM to use the GFX card built-in audio card,
    edit the ~hostpci0:~ line to include the whole PCIe group
    instead of just the one device in the group.
    Go from ~XX:XX.X~ to ~XX:XX~.
11. Start the VM and install GFX drivers
12. Install parsec for faster remote desktop


** Drive sanitization and imaging over the network
In this section of my notes, I walk through the process of performing a drive
sanitizaion and installation of new OS over the network. The goal of this process
is that scripts should do most of the work, and there should be minimal human interaction.
Ideally the only interaction I would like from a human is to plug in a network cable,
reboot the device, then interrupt the boot process to select the network boot option
instead of booting from local storage, and finally initiate the sanitization process.
*** DHCP server
When a device is trying to perform a network boot, it is reliant on the DHCP
server to provide a instructions on where to find a bootable image on the network.
In my lab environment, I've configured my DHCP server to provide the =NextServer=.
This by it self is not enough to retreive a bootable image. You must also configure
the DHCP server to provide a file name to request from the =NextServer=.
In my lab environment I've set =FileName= to be ~ipxe.efi~.
With these values set, any machine that is trying to perform a network boot will request
an IP address from the DHCP server, and receive and IP along with a location on the network
to download and run a bootable image. The bootable image could be some Windows or Linux OS,
but this is not very flexible because every machine that tries to perform a network boot
will use the same image. This is where iPXE comes in.
*** iPXE
iPXE is an open-source boot firmware. From my understanding, in the past, if you wanted to use
iPXE it needed to be burned into the NIC on the device you wanted to network boot. This is no
longer required, and it can now be chain loaded by the devices existing PXE environment.
This is great because now you can leverage all the features of iPXE on pretty much any network
bootable device. The two most useful for my intended use-case are scripting, and the ability to
load images over HTTP.

Normally bootable images loaded over the network are downloaded using the TFTP protocol, but
this is very slow because TFTP is limited to 512 byte packet size. Which is really slow when
you're trying to transfer a whole OS installation image. I do not currently know of a way to
completely avoid TFTP, but we can minimize the use of it by using iPXE. To get iPXE running on
a device, there must be a copy of it available on the network via TFTP. The device's built-in PXE
loader will then download and run the iPXE firmware. Once iPXE is loaded we now have the ability
to run scripts and download other images over HTTP.

One really nice feature of iPXE is that it can dynamically make HTTP requests. Here's an example
from ipxe.org:
#+begin_src shell
http://192.168.0.1/boot.php?mac=${net0/mac}&asset=${asset:uristring}
#+end_src
Which would then be evaluated to something like:
#+begin_src shell
http://192.168.0.1/boot.php?mac=52:54:00:12:34:56&asset=BKQ42M1
#+end_src
This is great because the HTTP server that is serving up the bootable images can supply a different
image based of the values it receives in the URL requested.

iPXE scripts can also be used to give menus. This could be useful if you wanted to select a specifc
image to boot. For example: maybe you'd like to boot a rescue image instead of sanitizing your drive
and reinstalling the OS. With menus you could interupt the script and do just that, assuming you've
got a rescue image availe on your HTTP server.

In my intended use-case I would script iPXE to boot a Windows installation image after so many seconds
by default. This is because after the drive sanitization is completed, there will be no OS installed on
the local storage and the device will be forced to use network booting. And since this process should be
automated as much as possible, it will need to boot into the Windows installation image automattically.
To initiate a sanitization of the drive the operator would need to select ShredOS from the menu shown.
*** ShredOS
ShredOS is a small (60 MB) Linux based OS with it's main purpose being to sanitize (shred) drives.
It can be preconfigured to perform a 'boot and nuke'. This will cause all storage devices attached
to be sanitized. ShredOS has options for a few different sanitization methods but the one I usually
stick with is the DoD 5220.22-M method, mostly because if it's good enough for the DoD surely it's
good enough for me. In addition to multiple sanitization methods, it can also write and upload log
files via FTP. Which is an essential feature for record-keeping.

